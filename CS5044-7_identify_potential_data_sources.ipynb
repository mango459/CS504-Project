{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Colllection and Conditioning\n",
    "Our datasets are distributed from several sources and require data collection techniques to integrate them into any potential analysis in a simplified manner.\n",
    "\n",
    "## Multifamily National File All Multifamily Properties By Units And Mortgages\n",
    "Our primary datasets are available on [fhfa.gov](https://www.fhfa.gov/data/multifamily-national-file-all-multifamily-properties-by-units-and-mortgages), but without an api will require the use of webscraping techniques and requests to integrate into the code. A set of data scraping tools specific to the webpage for this dataset has will be developed in `./src/srcaping.py` to address the findings of the exploration below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import itertools\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhfa_url = 'https://www.fhfa.gov/data/multifamily-national-file-all-multifamily-properties-by-units-and-mortgages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_data(url) -> BeautifulSoup:\n",
    "    \"\"\"Gets a pages data\n",
    "    Args:\n",
    "        n: int -> number representing the page number\n",
    "    Returns:\n",
    "        A bs4 object full of the page info\"\"\"\n",
    "    response = requests.get(url)\n",
    "    return BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 176 total anchor tags\n"
     ]
    }
   ],
   "source": [
    "# get main page html\n",
    "soup = get_page_data(fhfa_url)\n",
    "# parse for table data tags\n",
    "a_tags = soup.find_all('a')\n",
    "print(f'There are {len(a_tags)} total anchor tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/sites/default/files/2023-10/2020_MFNationalFile2020.zip\">[ZIP]</a>,\n",
       " <a href=\"/sites/default/files/2023-10/2020_Multifamily_National_File_Unit_Class-Level_Data.pdf\">[PDF]</a>,\n",
       " <a href=\"/sites/default/files/2023-10/2020_Multifamily_National_File_Property-Level_Data.pdf\">[PDF]</a>,\n",
       " <a href=\"/sites/default/files/2023-10/2019_MFNationalFile2019.zip\">​[ZIP]</a>,\n",
       " <a href=\"/sites/default/files/2023-10/2019_Multifamily_National_File_Unit_Class-Level_Data.pdf\">​[PDF]</a>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tags[120:125]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the some of the tags start off with the year then the zip, then two metadata pdfs. This can be used to extract relevant files and store them in the users `DATA_DIRECTORY`. We wont access the files from the web each time we run our analysis since the stability of scraped data is dependent on the webpage html staying static. What works now may not work in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sites/default/files/2024-08/2023_MFNationalFile2023.zip\n",
      "/sites/default/files/2024-08/2023_Multifamily_National_File_Unit_Class-Level_Data.pdf\n",
      "/sites/default/files/2024-08/2023_Multifamily_National_File_Property-Level_Data.pdf\n",
      "/sites/default/files/2023-09/2022_MFNationalFile2022.zip\n",
      "/sites/default/files/2023-09/2022_Multifamily_National_File_Unit_Class-Level_Data.pdf\n",
      "/sites/default/files/2023-09/2022_Multifamily_National_File_Property-Level_Data.pdf\n"
     ]
    }
   ],
   "source": [
    "counter = itertools.count()\n",
    "for a_tag in a_tags:\n",
    "    if a_tag['href'].endswith(('.pdf', '.zip')):\n",
    "        print(a_tag['href'])\n",
    "        if next(counter) == 5:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a way to hone in on the necessary data files and download them using requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scraping import get_fhfa_data, extract_zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_zips()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fhfa_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
